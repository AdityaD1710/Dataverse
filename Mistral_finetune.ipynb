{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/mindmate-3/mental_health_dataset_final.csv', encoding='latin-1')\n",
    "    print(\"Successfully read with latin-1 encoding.\")\n",
    "    # Proceed with your data analysis\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv('/kaggle/input/mindmate-3/mental_health_dataset_final.csv', encoding='cp1252')\n",
    "        print(\"Successfully read with cp1252 encoding.\")\n",
    "        # Proceed with your data analysis\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Failed to decode with both utf-8, latin-1, and cp1252. Further investigation needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df=df.drop('Unnamed: 2',axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(\"final_csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U huggingface_hub[cli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T18:15:39.585283Z",
     "iopub.status.busy": "2025-04-23T18:15:39.584929Z",
     "iopub.status.idle": "2025-04-23T18:15:39.618200Z",
     "shell.execute_reply": "2025-04-23T18:15:39.616558Z",
     "shell.execute_reply.started": "2025-04-23T18:15:39.585255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ff5ce8fdb24b199f24566c64ac3812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Call the login function and paste your token when prompted\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "\n",
    "# Check current login status\n",
    "user_info = whoami()\n",
    "print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Define file paths for Kaggle\n",
    "DATASET_URL = \"/kaggle/input/mindmate-3/mental_health_dataset_final.csv\"\n",
    "TRAIN_CSV = \"mental_health_train.csv\"\n",
    "TEST_CSV = \"mental_health_test.csv\"\n",
    "\n",
    "# Load the dataset safely (handle encoding issues)\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_URL, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(DATASET_URL, encoding='ISO-8859-1')\n",
    "\n",
    "# Print the initial size of the DataFrame\n",
    "print(f\"Initial number of rows before cleaning: {len(df)}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
    "\n",
    "# Clean text data\n",
    "df['Mental Health Concern'] = df['Mental Health Concern'].astype(str).str.strip()\n",
    "df['Professional Response'] = df['Professional Response'].astype(str).str.strip()\n",
    "\n",
    "# Define the size of the training and testing sets\n",
    "train_size = 1533\n",
    "test_size = 384\n",
    "\n",
    "# Ensure the dataset is large enough\n",
    "#if len(df) < (train_size + test_size):\n",
    " #   raise ValueError(\"Dataset is smaller than the combined training and testing sizes.\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:(train_size + test_size)]\n",
    "\n",
    "# Save the split data to Kaggle's working directory\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "# Output final confirmation\n",
    "print(f\"Training dataset size: {len(train_df)}\")\n",
    "print(f\"Testing dataset size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "train_concern_embeddings = embedding_model.encode(train_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "train_response_embeddings = embedding_model.encode(train_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "test_concern_embeddings = embedding_model.encode(test_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "test_response_embeddings = embedding_model.encode(test_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "\n",
    "np.save(\"train_concern_embeddings.npy\", train_concern_embeddings)\n",
    "np.save(\"train_response_embeddings.npy\", train_response_embeddings)\n",
    "np.save(\"test_concern_embeddings.npy\", test_concern_embeddings)\n",
    "np.save(\"test_response_embeddings.npy\", test_response_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T00:59:18.932568Z",
     "iopub.status.busy": "2025-04-21T00:59:18.932176Z",
     "iopub.status.idle": "2025-04-21T00:59:31.618622Z",
     "shell.execute_reply": "2025-04-21T00:59:31.617722Z",
     "shell.execute_reply.started": "2025-04-21T00:59:18.932540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run this FIRST\n",
    "!pip install -U transformers peft accelerate datasets sentencepiece\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T00:50:35.968402Z",
     "iopub.status.busy": "2025-04-21T00:50:35.968062Z",
     "iopub.status.idle": "2025-04-21T00:50:47.929630Z",
     "shell.execute_reply": "2025-04-21T00:50:47.928533Z",
     "shell.execute_reply.started": "2025-04-21T00:50:35.968378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T00:59:52.585781Z",
     "iopub.status.busy": "2025-04-21T00:59:52.585394Z",
     "iopub.status.idle": "2025-04-21T00:59:52.590159Z",
     "shell.execute_reply": "2025-04-21T00:59:52.589285Z",
     "shell.execute_reply.started": "2025-04-21T00:59:52.585752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T00:50:14.288814Z",
     "iopub.status.busy": "2025-04-21T00:50:14.288303Z",
     "iopub.status.idle": "2025-04-21T00:50:17.973365Z",
     "shell.execute_reply": "2025-04-21T00:50:17.972468Z",
     "shell.execute_reply.started": "2025-04-21T00:50:14.288776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes accelerate transformers peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T01:09:27.341435Z",
     "iopub.status.busy": "2025-04-21T01:09:27.341039Z",
     "iopub.status.idle": "2025-04-21T01:09:30.664161Z",
     "shell.execute_reply": "2025-04-21T01:09:30.663266Z",
     "shell.execute_reply.started": "2025-04-21T01:09:27.341401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -q optimum[onnxruntime]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T18:57:11.710274Z",
     "iopub.status.busy": "2025-04-22T18:57:11.710001Z",
     "iopub.status.idle": "2025-04-22T18:57:29.961330Z",
     "shell.execute_reply": "2025-04-22T18:57:29.960427Z",
     "shell.execute_reply.started": "2025-04-22T18:57:11.710252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes==0.41.1 accelerate==0.27.2 peft==0.9.0 transformers==4.39.3 datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:15:54.812078Z",
     "iopub.status.busy": "2025-04-20T19:15:54.811663Z",
     "iopub.status.idle": "2025-04-20T19:15:54.816341Z",
     "shell.execute_reply": "2025-04-20T19:15:54.815519Z",
     "shell.execute_reply.started": "2025-04-20T19:15:54.812048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:15:57.831524Z",
     "iopub.status.busy": "2025-04-20T19:15:57.831204Z",
     "iopub.status.idle": "2025-04-20T19:15:57.835497Z",
     "shell.execute_reply": "2025-04-20T19:15:57.834446Z",
     "shell.execute_reply.started": "2025-04-20T19:15:57.831498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Constants and Paths ---\n",
    "DATASET_PATH = \"/kaggle/input/mindmate-3/mental_health_dataset_final.csv\"\n",
    "TRAIN_CSV = \"mental_health_train.csv\"\n",
    "TEST_CSV = \"mental_health_test.csv\"\n",
    "HF_TOKEN = \"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\"  # Replace with Kaggle secret or env variable\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:02.199729Z",
     "iopub.status.busy": "2025-04-20T19:16:02.199434Z",
     "iopub.status.idle": "2025-04-20T19:16:02.244872Z",
     "shell.execute_reply": "2025-04-20T19:16:02.244227Z",
     "shell.execute_reply.started": "2025-04-20T19:16:02.199706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Load and Clean Data ---\n",
    "df = pd.read_csv(DATASET_PATH, encoding='latin1')\n",
    "df = df.drop_duplicates()\n",
    "df['Mental Health Concern'] = df['Mental Health Concern'].fillna('').str.strip()\n",
    "df['Professional Response'] = df['Professional Response'].fillna('').str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:05.197525Z",
     "iopub.status.busy": "2025-04-20T19:16:05.197221Z",
     "iopub.status.idle": "2025-04-20T19:16:05.216326Z",
     "shell.execute_reply": "2025-04-20T19:16:05.215471Z",
     "shell.execute_reply.started": "2025-04-20T19:16:05.197501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_size, test_size = 1533, 384\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:(train_size + test_size)]\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:09.060491Z",
     "iopub.status.busy": "2025-04-20T19:16:09.060207Z",
     "iopub.status.idle": "2025-04-20T19:16:09.316673Z",
     "shell.execute_reply": "2025-04-20T19:16:09.316057Z",
     "shell.execute_reply.started": "2025-04-20T19:16:09.060469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Hugging Face Login ---\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:12.077689Z",
     "iopub.status.busy": "2025-04-20T19:16:12.077296Z",
     "iopub.status.idle": "2025-04-20T19:16:12.186588Z",
     "shell.execute_reply": "2025-04-20T19:16:12.185669Z",
     "shell.execute_reply.started": "2025-04-20T19:16:12.077649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Apply LoRA ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:14.881562Z",
     "iopub.status.busy": "2025-04-20T19:16:14.881269Z",
     "iopub.status.idle": "2025-04-20T19:16:18.663312Z",
     "shell.execute_reply": "2025-04-20T19:16:18.662631Z",
     "shell.execute_reply.started": "2025-04-20T19:16:14.881540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Generate Sentence Embeddings ---\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_concern_embeddings = embedding_model.encode(train_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "train_response_embeddings = embedding_model.encode(train_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "test_concern_embeddings = embedding_model.encode(test_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "test_response_embeddings = embedding_model.encode(test_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "\n",
    "np.save(\"train_concern_embeddings.npy\", train_concern_embeddings)\n",
    "np.save(\"train_response_embeddings.npy\", train_response_embeddings)\n",
    "np.save(\"test_concern_embeddings.npy\", test_concern_embeddings)\n",
    "np.save(\"test_response_embeddings.npy\", test_response_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:29.694233Z",
     "iopub.status.busy": "2025-04-20T19:16:29.693911Z",
     "iopub.status.idle": "2025-04-20T19:16:29.698663Z",
     "shell.execute_reply": "2025-04-20T19:16:29.697875Z",
     "shell.execute_reply.started": "2025-04-20T19:16:29.694206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Dataset Tokenization ---\n",
    "def tokenize_function(batch):\n",
    "    inputs = tokenizer(\n",
    "        [str(x) for x in batch[\"Mental Health Concern\"]],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [str(x) for x in batch[\"Professional Response\"]],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:32.869834Z",
     "iopub.status.busy": "2025-04-20T19:16:32.869503Z",
     "iopub.status.idle": "2025-04-20T19:16:34.063338Z",
     "shell.execute_reply": "2025-04-20T19:16:34.062490Z",
     "shell.execute_reply.started": "2025-04-20T19:16:32.869771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('csv', data_files={'train': TRAIN_CSV})['train']\n",
    "test_dataset = load_dataset('csv', data_files={'test': TEST_CSV})['test']\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:40.414348Z",
     "iopub.status.busy": "2025-04-20T19:16:40.414055Z",
     "iopub.status.idle": "2025-04-20T19:16:40.441371Z",
     "shell.execute_reply": "2025-04-20T19:16:40.440640Z",
     "shell.execute_reply.started": "2025-04-20T19:16:40.414326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:16:47.134321Z",
     "iopub.status.busy": "2025-04-20T19:16:47.134010Z",
     "iopub.status.idle": "2025-04-20T19:16:47.145732Z",
     "shell.execute_reply": "2025-04-20T19:16:47.144825Z",
     "shell.execute_reply.started": "2025-04-20T19:16:47.134294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:23:26.018300Z",
     "iopub.status.busy": "2025-04-20T19:23:26.018009Z",
     "iopub.status.idle": "2025-04-20T19:23:26.028307Z",
     "shell.execute_reply": "2025-04-20T19:23:26.027575Z",
     "shell.execute_reply.started": "2025-04-20T19:23:26.018279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:19:37.101917Z",
     "iopub.status.busy": "2025-04-20T19:19:37.101557Z",
     "iopub.status.idle": "2025-04-20T19:19:37.116660Z",
     "shell.execute_reply": "2025-04-20T19:19:37.115769Z",
     "shell.execute_reply.started": "2025-04-20T19:19:37.101890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:21:32.332125Z",
     "iopub.status.busy": "2025-04-20T19:21:32.331833Z",
     "iopub.status.idle": "2025-04-20T19:21:32.336592Z",
     "shell.execute_reply": "2025-04-20T19:21:32.335743Z",
     "shell.execute_reply.started": "2025-04-20T19:21:32.332104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:25:03.624983Z",
     "iopub.status.busy": "2025-04-20T19:25:03.624648Z",
     "iopub.status.idle": "2025-04-20T19:25:03.687916Z",
     "shell.execute_reply": "2025-04-20T19:25:03.687094Z",
     "shell.execute_reply.started": "2025-04-20T19:25:03.624956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.requires_grad)  # Should be True for parameters you want to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:21:54.724056Z",
     "iopub.status.busy": "2025-04-20T19:21:54.723703Z",
     "iopub.status.idle": "2025-04-20T19:21:54.749904Z",
     "shell.execute_reply": "2025-04-20T19:21:54.748802Z",
     "shell.execute_reply.started": "2025-04-20T19:21:54.724027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # If you want all parameters to be trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:29:19.058958Z",
     "iopub.status.busy": "2025-04-20T19:29:19.058628Z",
     "iopub.status.idle": "2025-04-20T19:29:19.065530Z",
     "shell.execute_reply": "2025-04-20T19:29:19.064594Z",
     "shell.execute_reply.started": "2025-04-20T19:29:19.058932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "params[0].requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:29:22.550388Z",
     "iopub.status.busy": "2025-04-20T19:29:22.550111Z",
     "iopub.status.idle": "2025-04-20T19:29:22.595585Z",
     "shell.execute_reply": "2025-04-20T19:29:22.594786Z",
     "shell.execute_reply.started": "2025-04-20T19:29:22.550368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Parameter {name} does not require gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T19:26:52.148305Z",
     "iopub.status.busy": "2025-04-20T19:26:52.148020Z",
     "iopub.status.idle": "2025-04-20T19:26:52.833128Z",
     "shell.execute_reply": "2025-04-20T19:26:52.831897Z",
     "shell.execute_reply.started": "2025-04-20T19:26:52.148284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Train ---\n",
    "trainer.train()\n",
    "trainer.save_model(\"./mistral_finetuned\")\n",
    "print(\"âœ… Training complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T18:57:33.399876Z",
     "iopub.status.busy": "2025-04-22T18:57:33.399538Z",
     "iopub.status.idle": "2025-04-23T00:57:32.394338Z",
     "shell.execute_reply": "2025-04-23T00:57:32.393330Z",
     "shell.execute_reply.started": "2025-04-22T18:57:33.399847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- Constants and Paths ---\n",
    "DATASET_PATH = \"/kaggle/input/mindmate-3/mental_health_dataset_final.csv\"\n",
    "TRAIN_CSV = \"mental_health_train.csv\"\n",
    "TEST_CSV = \"mental_health_test.csv\"\n",
    "HF_TOKEN = \"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\"  # Replace with Kaggle secret or env variable\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# --- Load and Clean Data ---\n",
    "df = pd.read_csv(DATASET_PATH, encoding='latin1')\n",
    "df = df.drop_duplicates()\n",
    "df['Mental Health Concern'] = df['Mental Health Concern'].fillna('').str.strip()\n",
    "df['Professional Response'] = df['Professional Response'].fillna('').str.strip()\n",
    "\n",
    "# Add prompt formatting to create proper instruction tuning data\n",
    "df['formatted_text'] = \"Mental Health Concern: \" + df['Mental Health Concern'] + \"\\n\\nProfessional Response: \" + df['Professional Response']\n",
    "\n",
    "# Split data\n",
    "train_size, test_size = 1533, 384\n",
    "train_df = df.iloc[:train_size]\n",
    "test_df = df.iloc[train_size:(train_size + test_size)]\n",
    "\n",
    "train_df.to_csv(TRAIN_CSV, index=False)\n",
    "test_df.to_csv(TEST_CSV, index=False)\n",
    "\n",
    "# --- Generate Sentence Embeddings ---\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train_concern_embeddings = embedding_model.encode(train_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "train_response_embeddings = embedding_model.encode(train_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "test_concern_embeddings = embedding_model.encode(test_df['Mental Health Concern'].tolist(), show_progress_bar=True)\n",
    "test_response_embeddings = embedding_model.encode(test_df['Professional Response'].tolist(), show_progress_bar=True)\n",
    "\n",
    "np.save(\"train_concern_embeddings.npy\", train_concern_embeddings)\n",
    "np.save(\"train_response_embeddings.npy\", train_response_embeddings)\n",
    "np.save(\"test_concern_embeddings.npy\", test_concern_embeddings)\n",
    "np.save(\"test_response_embeddings.npy\", test_response_embeddings)\n",
    "\n",
    "# --- Hugging Face Login ---\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# --- Load Tokenizer and Quantized Model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Clear CUDA cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training - THIS IS THE KEY FIX\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Ensure model is in training mode\n",
    "model.config.use_cache = False  # This is important for training LLMs\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# --- Apply LoRA ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters to verify LoRA setup\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "def prepare_dataset(df):\n",
    "    formatted_dataset = {\n",
    "        \"text\": df[\"formatted_text\"].tolist()\n",
    "    }\n",
    "    return formatted_dataset\n",
    "\n",
    "train_data = prepare_dataset(train_df)\n",
    "test_data = prepare_dataset(test_df)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# --- Tokenization Function ---\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts with proper padding and truncation\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Increased max length to fit both question and answer\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Important: Set labels equal to input_ids for causal language modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors format\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    # Make sure these are set appropriately:\n",
    "    remove_unused_columns=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    # Add gradient checkpointing to save memory\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# --- Trainer Setup ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "try:\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./mistral_finetuned\")\n",
    "    print(\"âœ… Training complete. Model saved.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    \n",
    "    # Try to save what we have so far\n",
    "    try:\n",
    "        trainer.save_model(\"./mistral_finetuned_partial\")\n",
    "        print(\"Partial model saved despite error.\")\n",
    "    except:\n",
    "        print(\"Could not save partial model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T01:12:13.015842Z",
     "iopub.status.busy": "2025-04-23T01:12:13.015075Z",
     "iopub.status.idle": "2025-04-23T01:12:18.184017Z",
     "shell.execute_reply": "2025-04-23T01:12:18.183158Z",
     "shell.execute_reply.started": "2025-04-23T01:12:13.015803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r mistral_finetuned.zip ./mistral_finetuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:41:25.349468Z",
     "iopub.status.busy": "2025-04-23T16:41:25.349180Z",
     "iopub.status.idle": "2025-04-23T16:41:32.112514Z",
     "shell.execute_reply": "2025-04-23T16:41:32.111047Z",
     "shell.execute_reply.started": "2025-04-23T16:41:25.349443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:48:35.604337Z",
     "iopub.status.busy": "2025-04-23T16:48:35.603990Z",
     "iopub.status.idle": "2025-04-23T16:48:35.626588Z",
     "shell.execute_reply": "2025-04-23T16:48:35.625357Z",
     "shell.execute_reply.started": "2025-04-23T16:48:35.604311Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0908142a3546ea8fbf20d980d33dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/model\",\n",
    "    repo_id=\"pointbreak3000/Mistral-7B_Mental_Health\",\n",
    "    repo_type=\"model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:50:02.340045Z",
     "iopub.status.busy": "2025-04-23T16:50:02.339640Z",
     "iopub.status.idle": "2025-04-23T16:50:14.477690Z",
     "shell.execute_reply": "2025-04-23T16:50:14.476023Z",
     "shell.execute_reply.started": "2025-04-23T16:50:02.340014Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './mistral_finetuned'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './mistral_finetuned'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-54aa521d959f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./mistral_finetuned\"\u001b[0m  \u001b[0;31m# wherever you saved your model after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './mistral_finetuned'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"./mistral_finetuned\"  # wherever you saved your model after training\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T17:05:42.911338Z",
     "iopub.status.busy": "2025-04-23T17:05:42.910980Z",
     "iopub.status.idle": "2025-04-23T17:05:42.918137Z",
     "shell.execute_reply": "2025-04-23T17:05:42.916562Z",
     "shell.execute_reply.started": "2025-04-23T17:05:42.911312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-a4720ae7fa1e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a4720ae7fa1e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kaggle datasets download -d AdityaUD2501/mistral_finetuned\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "kaggle datasets download -d AdityaUD2501/mistral_finetuned\n",
    "unzip mistral_finetuned.zip -d mistral-mental-health-lora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T18:15:58.339762Z",
     "iopub.status.busy": "2025-04-23T18:15:58.339320Z",
     "iopub.status.idle": "2025-04-23T18:18:17.741111Z",
     "shell.execute_reply": "2025-04-23T18:18:17.734807Z",
     "shell.execute_reply.started": "2025-04-23T18:15:58.339725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78354eb48d104054b970a17be8814874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8529069ea9274de39fe32965ee41b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549af919d78548d784b06f797ca4f54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8d1f5af8d0436489c9b7b2980df734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37c936d2edc4f8682c3450a214db610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6632081de184d0089e5423e2367c295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48daa6aee7949098400187540c12744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a787acbdf548d99a43c307abf98d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5331c986f99a4d60b42c7e3e7ea8fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2204221f7afc4bc5b0a3825c1349b8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71782758044b4111b4910a0d26e4c8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'pointbreak3000/mistral-mental-health-lora'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/pointbreak3000/mistral-mental-health-lora/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    258\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    457\u001b[0m             )\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68092ee9-6ce8a7617bb9b8e43ca83bce;07942e09-512b-4aa3-a0dd-21297ea70df8)\n\nRepository Not Found for url: https://huggingface.co/pointbreak3000/mistral-mental-health-lora/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db8f46d421be>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the fine-tuned model with PEFT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pointbreak3000/mistral-mental-health-lora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Define the prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 479\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    480\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'pointbreak3000/mistral-mental-health-lora'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "# Load the fine-tuned model with PEFT\n",
    "model = PeftModel.from_pretrained(base_model, \"pointbreak3000/mistral-mental-health-lora\")\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"I'm feeling stressed and anxious lately. What should I do?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode and print the response\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7025446,
     "sourceId": 11244104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7155014,
     "isSourceIdPinned": false,
     "sourceId": 11424532,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
